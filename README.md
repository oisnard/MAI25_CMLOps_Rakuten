MAI25_CMLOPS : project Rakuten 
==============================

Projet pÃ©dagogique rÃ©alisÃ© dans le cadre de la formation MLOps de DataScientest (Cohorte MAI 2025), axÃ© sur la mise en place dâ€™une architecture MLOps complÃ¨te pour le traitement et la classification de donnÃ©es produits Rakuten dans le cadre du challenge ens-data : https://challengedata.ens.fr/participants/challenges/35/ .
Les modÃ¨les dÃ©ployÃ©s sont inspirÃ©s de ceux dÃ©finis par l'Ã©quipe Olivier ISNARD / Julien TREVISAN / LoÃ¯c RAMAYE lors de leur formation Data Scientist (cohorte Juin 2024) et qui avaient permis d'obtenir la premiÃ¨re place au classement public et privÃ© du challenge. Dans le cadre de ce projet, des modÃ¨les plus lÃ©gers ont Ã©tÃ© mises en place afin de rÃ©duire les coÃ»ts d'instance AWS (dÃ©ploiement sur instance EC2 avec GPU).


---
## ğŸš€ Objectifs rÃ©alisÃ©s 

- Construire un pipeline ML complet (data loading â†’ preprocessing â†’ entraÃ®nement â†’ Ã©valuation â†’ dÃ©ploiement) avec **Airflow**  
- Suivi des expÃ©riences via **MLflow**  
- Suivi du drift des donnÃ©es avec **Evidently**
- Conteneuriser les composants (Docker)  
- Fournir une API REST prÃ©dictive sÃ©curisÃ©e et dÃ©ployÃ©e sur K8S (avec scalability)
- Versionner les donnÃ©es avec **DVC**  
- Mettre en place des tests unitaires et une intÃ©gration continue (CI) via **GitHub Actions** pour garantir la fiabilitÃ© du pipeline

---
## ğŸ§­ SchÃ©ma dâ€™Architecture MLOps simplifiÃ©
![alt text](/data/dataviz/schema_archi.png)

---
## ğŸ“ Structure du dÃ©pÃ´t

```bash
.
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/                  # DAGs Airflow (full pipeline, datastreams, trafic)
â”‚   â””â”€â”€ plugins/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # images + CSV ENS Data (DVC)
â”‚   â”œâ”€â”€ processed/             # jeux train/val/test, features, prÃ©dictions (DVC)
â”‚   â”œâ”€â”€ stream/{raw,processed} # flux simulÃ©s
â”‚   â”œâ”€â”€ monitoring_sample/     # Ã©chantillons pour data/drift monitoring
â”‚   â””â”€â”€ dataviz/               # schÃ©mas & assets pour streamlit
â”œâ”€â”€ docker/ â€¦                  # Dockerfiles (api, airflow, train{_gpu}, preprocess, features{_gpu}, evaluate{_gpu}, mlflow, streamlit, evidently, datastreams, traffic) + prometheus/grafana + requirements
â”œâ”€â”€ k8s/ â€¦                     # manifests K8s (deployment/service/ingress/hpa, PV/PVC, monitoring Prometheus/Grafana, RBAC, templates)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                   # FastAPI (JWT, middleware, service)
â”‚   â”œâ”€â”€ data/                  # import/make_dataset, preprocessing, datastreams
â”‚   â”œâ”€â”€ features/              # build_features
â”‚   â”œâ”€â”€ models/                # train/evaluate/predict (texte, image, fusion)
â”‚   â”œâ”€â”€ streamlit/             # app Streamlit
â”‚   â”œâ”€â”€ tools/                 # utilitaires (rÃ©seau, datastream)
â”‚   â”œâ”€â”€ traffic_generation/    # Generation de requÃªtes API pour tester scale up/down K3S api.
â”‚   â””â”€â”€ visualization/         # fonctions de visualisation
â”œâ”€â”€ tests/                     # Tests unitaires pour la CI
â”œâ”€â”€ models/                    # artefacts modÃ¨les (poids versionnÃ©s via DVC)
â”œâ”€â”€ metrics/                   # rapports de classification (CSV/JSON)
â”œâ”€â”€ monitoring/utils/          # scripts Evidently (drift report)
â”œâ”€â”€ docker-compose.template.yml
â”œâ”€â”€ params.yaml                # ParamÃ¨tres globaux pour le pipeline (modÃ¨le, seed, split, etc.)
â”œâ”€â”€ pytest.ini
â”œâ”€â”€ scripts/                   # CI locale, k3s deploy/cleanup, monitoring, checks
â”œâ”€â”€ generate_compose.sh        # Pour gÃ©nÃ©rer le manifeste docker-compose.yml
â”œâ”€â”€ setup.py
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .env                      # Variables dâ€™environnement (ex: BASE_DIR)
â””â”€â”€ README.md                 # Documentation du projet (ce fichier)
```
ğŸ” Fichiers de configuration importants
| Fichier              | RÃ´le                                                           |
| -------------------- | -------------------------------------------------------------- |
| `docker-compose.yml` | Lance tous les services nÃ©cessaires (Airflow, API, MLflowâ€¦)    |
| `params.yaml`        | Centralise les hyperparamÃ¨tres, chemins, splits, etc.          |
| `processed.dvc`      | Suit les transformations de donnÃ©es via DVC                    |
| `.dvcignore`         | Exclut certains fichiers des suivis DVC                        |
| `.env`               | DÃ©finit les variables dâ€™environnement Docker (base\_dir, etc.) |
<br>Pour gÃ©nÃ©rer le fichier docker-compose.yml Ã  partir du `docker-compose-template.yml`, il faut exÃ©cuter la commande : 
```
./generate_compose.sh
```
---
### ğŸ§° Services
| Service     | Port  | Description                            |
| ----------- | ----- | ---------------------------------------|
| Airflow UI  | 8080  | Orchestration du pipeline              |
| MLflow      | 5000  | Tracking des expÃ©riences               |
| API FastAPI | 8000  | Endpoint de prÃ©diction                 |
| Evidently   | 9000  | Suivi du data drift                    |
| Prometheus  | 30900 | Monitoring des mÃ©triques API           |
| Grafana     | 30300 | Visualisation des mÃ©triques Prometheus |
| Streamlit   | 8501  | Application streamlit - Projet         |


---
### â–¶ï¸ Lancer lâ€™environnement
1. PrÃ©requis

    Docker installÃ©

    CrÃ©er un fichier .env basÃ© sur .env_template. 

2. Lancement des services
    ```
    ./script/deploy_k3s.sh     # DÃ©ploiement de l'api sur pod K3S
    ./script/deploy_monitoring.sh # DÃ©ploiement du monitoring de l'API via Prometheus/Grafana 
    docker compose up --build  # Lancer les autres services (airflow, mlflow, evidently, streamlit)
    ```
    Airflow sera accessible sur localhost:8080, et MLflow sur localhost:5000.
    Prometheus sera accessible sur http://localhost:90900 (localhost in case of local or EC2 public IP address))
    (Permet de visualiser les mÃ©triques exposÃ©es par lâ€™API ou Airflow via /metrics)
    Grafana sera accessible sur http://localhost:30300 (Identifiants par dÃ©faut : admin / admin) (localhost in case of local or EC2 public IP address)

---
### âš™ï¸ Pipelines Airflow

Le DAG principal (rakuten_dags.py) orchestre les Ã©tapes suivantes :
- data_loading_task
- preprocessing_task
- training_task
- evaluation_task
- mlflow_dag_task

---
### ğŸ§ª Suivi des expÃ©riences

Les logs et artefacts d'entraÃ®nement sont automatiquement tracÃ©s dans MLflow :

```http://localhost:5000 ```

Le tracking inclut :
- mÃ©triques dâ€™Ã©valuation
- paramÃ¨tres du modÃ¨le
- visualisations

---
### ğŸŒ API de prÃ©diction

Un service FastAPI expose un endpoint sur le port 8000.

---
### ğŸ§ª Tests et reproductibilitÃ©

Les composants sont encapsulÃ©s dans des images Docker distinctes pour chaque Ã©tape (dataloading, preprocessing, etc.), facilitant lâ€™isolation et les tests.

---
### ğŸ—‚ï¸ Gestion des donnÃ©es avec DVC

Les transformations sont suivies avec DVC pour permettre une versioning des datasets transformÃ©s.
```
dvc repro
dvc push
``` 

---
### ğŸ“Š Monitoring avec Prometheus et Grafana
Lâ€™API FastAPI est instrumentÃ©e avec `prometheus_fastapi_instrumentator` pour exposer des mÃ©triques accessibles sur :

```
http://localhost:8000/metrics
```

ğŸ” Prometheus

Prometheus collecte les mÃ©triques de lâ€™API toutes les 15 secondes.
Interface accessible via :

```
http://localhost:30900
```

Exemple de requÃªte PromQL Ã  exÃ©cuter dans lâ€™interface :

```
sum by (handler) (http_requests_total)
```

Cela permet de visualiser le nombre total de requÃªtes par endpoint.


ğŸ“ˆ Grafana

Grafana permet de crÃ©er des dashboards personnalisÃ©s Ã  partir des donnÃ©es Prometheus.
AccÃ¨s Ã  Grafana :

```
http://localhost:30300
```

Identifiants par dÃ©faut :
	â€¢	Login : admin
	â€¢	Mot de passe : admin

Pour configurer :
	1.	Aller dans â€œConnections > Data sourcesâ€.
	2.	Cliquer sur â€œAdd data sourceâ€.
	3.	SÃ©lectionner Prometheus.
	4.	Renseigner lâ€™URL : http://prometheus:30900.
	5.	CrÃ©er des panels Ã  partir de requÃªtes PromQL (ex: http_requests_total).

---
### âœ… Tests unitaires

Le rÃ©pertoire tests/ contient des tests unitaires pour valider les diffÃ©rentes Ã©tapes du pipeline ML : chargement des donnÃ©es, prÃ©traitement, entraÃ®nement, prÃ©diction, etc.
ğŸ“¦ Structure
```
tests/
â”œâ”€â”€ test_dataloading.py
â”œâ”€â”€ test_preprocessing.py
â”œâ”€â”€ test_training.py
â”œâ”€â”€ test_evaluation.py
â””â”€â”€ conftest.py  # Fixtures partagÃ©es 
```
â–¶ï¸ ExÃ©cution des tests

S'assurer dâ€™avoir lancÃ© l'instance EC2 avec GPU, puis exÃ©cuter :
```
./scripts/run_ci.sh
```

### Installation k3s
Lancer le script :
```
./scripts/install_k3s.sh
```

### DÃ©ployer l'api sur k3s et le monitoring
Lancer les scripts
```
./scripts/deploy_k3s.sh
./scripts/deploy_monitoring.sh
```

### Nettoyer les ressources k3s (namespace, pv/pvc, pods)
Lancer les scripts
```
./scripts/cleanup_monitoring.sh
./scripts/cleanup_k3s.sh
```

---
### ğŸ“ Auteurs

- Olivier ISNARD
- Christian SEGNOU

EncadrÃ©s dans le cadre de la formation MLOps par Maria de DataScientest.
