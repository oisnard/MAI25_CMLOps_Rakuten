import tensorflow as tf
import pandas as pd
import src.tools.tools as tools 
import src.models.models as models
from src.models.metrics import SparseF1Score
import os 
import logging 
import numpy as np
from sklearn.metrics import classification_report
import json


def load_dataset() -> tuple:
    """
    Load the test dataset generated by ./src/data/make_dataset.py.
    Returns:
        X_test (DataFrame): The test dataset containing features.
        y_test (DataFrame): The test dataset containing labels.
    """
    try:
        X_test = pd.read_csv(os.path.join(tools.DATA_PROCESSED_DIR, "X_test.csv"), index_col=0)
        y_test = pd.read_csv(os.path.join(tools.DATA_PROCESSED_DIR, "y_test.csv"), index_col=0)

    except FileNotFoundError as e:
        logging.error(f"File not found: {e}")
        raise
    except Exception as e:
        logging.error(f"An error occurred while loading datasets: {e}")
        raise
    # Ensure that the 'image_path' column exists in X_test
    if 'image_path' not in X_test.columns:
        logging.error("The 'image_path' column is missing in the X_test dataset.")
        raise ValueError("The 'image_path' column is missing in the X_test dataset.")
    # Ensure that the 'prdtypecode' column exists in y_test
    if 'prdtypecode' not in y_test.columns:
        logging.error("The 'prdtypecode' column is missing in the y_test dataset.")
        raise ValueError("The 'prdtypecode' column is missing in the y_test dataset.")
    # Ensure that the 'prdtypecode' column is of integer type
    if not pd.api.types.is_integer_dtype(y_test['prdtypecode']):
        logging.error("The 'prdtypecode' column must be of integer type.")
        raise ValueError("The 'prdtypecode' column must be of integer type.")
    # Ensure that the 'image_path' column is of string type
    if not pd.api.types.is_string_dtype(X_test['image_path']):
        logging.error("The 'image_path' column must be of string type.")
        raise ValueError("The 'image_path' column must be of string type.")
    return X_test, y_test

def main():
    # Set up logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # Load dataset parameters from YAML file
    params = tools.load_dataset_params_from_yaml()

    logger.info("Loading dataset for prediction...")
    # Load the X_test dataset
    X_test, y_test = load_dataset()
    # Check if the dataset was loaded successfully
    if X_test is None or y_test is None:
        logger.error("Failed to load dataset.")
        exit(1)

    # Load dataset parameters from YAML file
    params = tools.load_dataset_params_from_yaml()    

    # Load the number of classes, the number of trainable layers, batch_size and nb of epochs from params
    num_classes = tools.NUM_CLASSES
    nb_trainable_layers = params['models_parameters']['EfficientNetB1']['nb_trainable_layers']
    if not isinstance(nb_trainable_layers, int) or nb_trainable_layers < 0:
        logging.error(f"Invalid nb_trainable_layers value: {nb_trainable_layers}. It should be a non-negative integer.")
        raise ValueError(f"Invalid nb_trainable_layers value: {nb_trainable_layers}. It should be a non-negative integer.")

    # Load BATCH_SIZE from params
    BATCH_SIZE = params['training_parameters']['batch_size']
    if not isinstance(BATCH_SIZE, int) or BATCH_SIZE <= 0:
        logging.error(f"Invalid BATCH_SIZE value: {BATCH_SIZE}. It should be a positive integer.")
        raise ValueError(f"Invalid BATCH_SIZE value: {BATCH_SIZE}. It should be a positive integer.")



    logging.info("Creating TensorFlow datasets for testing...")
    # Convert the training and validation datasets to TensorFlow datasets
    test_dataset = tf.data.Dataset.from_tensor_slices((X_test.image_path.tolist(), y_test.prdtypecode.tolist()))
    test_dataset = test_dataset.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
   # Check if the datasets are not empty
    if test_dataset is None:
        logging.error("test_dataset is empty.")
        raise ValueError("test_dataset is empty.")
    logging.info("TensorFlow dataset created successfully.")

    # Load the EfficientNetB1 model
    model = models.build_model_image_efficientNetB1(num_classes=num_classes, 
                                                    nb_trainable_layers=nb_trainable_layers)   
    if model is None:
        logging.error("Failed to load EfficientNetB1 model.")
        raise ValueError("Failed to load EfficientNetB1 model.")
    
    if not os.path.exists(tools.MODEL_DIR):
        logging.error(f"Model directory does not exist: {tools.MODEL_DIR}")
        raise FileNotFoundError(f"Model directory does not exist: {tools.MODEL_DIR}")

    model.load_weights(os.path.join(tools.MODEL_DIR, "efficientNetB1_model.weights.h5"))
    logging.info("Model weights loaded successfully.")

    # Make predictions
    logging.info("Making predictions on the test dataset...")
    predictions = model.predict(test_dataset)
    logging.info("Predictions made successfully.")

    # Convert predictions to class labels
    y_pred = np.argmax(predictions, -1)
    logging.info("Predicted classes obtained successfully.")

    # Convert y_test to numpy array for evaluation
    # Load the mapping dictionary to convert integer labels back to prdtypecode
    dict_mapping_reverse = tools.load_reverse_mapping_dict()
    if dict_mapping_reverse is None:
        logging.error("Failed to load the reverse mapping dictionary.")
        raise ValueError("Failed to load the reverse mapping dictionary.")
    # Convert integer predictions to prdtypecode using the reverse mapping dictionary
    y_pred_prdtypecode = [dict_mapping_reverse.get(pred, "Unknown") for pred in y_pred]
    logging.info("Converted predicted classes to prdtypecode successfully.")

    # Convert y_test to numpy array for evaluation
    y_test_prdtypecode = y_test.prdtypecode.tolist()
    y_test_prdtypecode = [dict_mapping_reverse.get(label, "Unknown") for label in y_test_prdtypecode]
    report_dict = classification_report(y_test_prdtypecode, y_pred_prdtypecode, output_dict=True, zero_division=0)
    logging.info("Classification report generated successfully.")
    # Convert report to DataFrame for better visualization

    report_df = pd.DataFrame(report_dict).transpose()
    # Save the classification report to a CSV file
    os.makedirs(tools.METRICS_DIR, exist_ok=True)  # Ensure the metrics directory exists
    report_path = os.path.join(tools.METRICS_DIR, "classification_report_model_img.csv")
    report_df.to_csv(report_path)
    logging.info(f"Classification report saved to {report_path}")
    logging.info("Prediction process completed successfully.")

    
    # Save the classification report to a JSON file
    logging.info("Saving classification report to JSON file...")
    # Save the classification report as a JSON file
    report_json_path = os.path.join(tools.METRICS_DIR, "classification_report_model_img.json")
    with open(report_json_path, "w", encoding="utf-8") as f:
        json.dump(report_dict, f, ensure_ascii=True, indent=4)
    logging.info(f"Classification report saved to {report_json_path}")    

if __name__ == "__main__":
    main()
else:
    import inspect

    # If this script is imported, run the main function only if it is called from evaluate_model.py
    stack = inspect.stack()
    for frame in stack:
        if "evaluate_model.py" in frame.filename:
            main()
            break