MAI25_CMLOPS: Rakuten Project 
==============================

Educational project carried out as part of the **MLOps training** at DataScientest (Cohort MAI 2025), focused on implementing a complete MLOps architecture for processing and classifying Rakuten product data in the context of the ENS Data Challenge: https://challengedata.ens.fr/participants/challenges/35/.  
The deployed models are inspired by those defined by the team Olivier ISNARD / Julien TREVISAN / LoÃ¯c RAMAYE during their Data Scientist training (June 2024 cohort), which achieved 1st place in both the public and private leaderboards of the challenge.  
In this project, lighter models were implemented in order to reduce AWS instance costs (deployment on EC2 with GPU).  

---
## ğŸš€ Objectives achieved

- Build a complete ML pipeline (data loading â†’ preprocessing â†’ training â†’ evaluation â†’ deployment) with **Airflow**  
- Track experiments with **MLflow**  
- Monitor data drift with **Evidently**  
- Containerize components with **Docker**  
- Provide a secure predictive REST API deployed on **Kubernetes (K8s)** with scalability  
- Version datasets with **DVC**  
- Implement unit tests and **continuous integration (CI)** with **GitHub Actions** to ensure pipeline reliability  

---
## ğŸ§­ Simplified MLOps Architecture Diagram
![alt text](/data/dataviz/schema_archi.png)

---
## ğŸ“ Repository structure

```bash
.
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/                  # Airflow DAGs (full pipeline, datastreams, traffic)
â”‚   â””â”€â”€ plugins/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # ENS Data CSVs + images (DVC)
â”‚   â”œâ”€â”€ processed/             # train/val/test sets, features, predictions (DVC)
â”‚   â”œâ”€â”€ stream/{raw,processed} # simulated streams
â”‚   â”œâ”€â”€ monitoring_sample/     # samples for data/drift monitoring
â”‚   â””â”€â”€ dataviz/               # diagrams & assets for streamlit
â”œâ”€â”€ docker/ â€¦                  # Dockerfiles (api, airflow, train{_gpu}, preprocess, features{_gpu}, evaluate{_gpu}, mlflow, streamlit, evidently, datastreams, traffic) + prometheus/grafana + requirements
â”œâ”€â”€ k8s/ â€¦                     # K8s manifests (deployment/service/ingress/hpa, PV/PVC, Prometheus/Grafana monitoring, RBAC, templates)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                   # FastAPI (JWT, middleware, service)
â”‚   â”œâ”€â”€ data/                  # import/make_dataset, preprocessing, datastreams
â”‚   â”œâ”€â”€ features/              # build_features
â”‚   â”œâ”€â”€ models/                # train/evaluate/predict (text, image, fusion)
â”‚   â”œâ”€â”€ streamlit/             # Streamlit app
â”‚   â”œâ”€â”€ tools/                 # utilities (network, datastream)
â”‚   â”œâ”€â”€ traffic_generation/    # API request generator for K3S scaling tests
â”‚   â””â”€â”€ visualization/         # visualization functions
â”œâ”€â”€ tests/                     # Unit tests for CI
â”œâ”€â”€ models/                    # model artifacts (weights tracked via DVC)
â”œâ”€â”€ metrics/                   # classification reports (CSV/JSON)
â”œâ”€â”€ monitoring/utils/          # Evidently scripts (drift reports)
â”œâ”€â”€ docker-compose.template.yml
â”œâ”€â”€ params.yaml                # Global pipeline parameters (model, seed, split, etc.)
â”œâ”€â”€ pytest.ini
â”œâ”€â”€ scripts/                   # Local CI, k3s deploy/cleanup, monitoring, checks
â”œâ”€â”€ generate_compose.sh        # Generate docker-compose.yml from template
â”œâ”€â”€ setup.py
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .env                       # Environment variables (ex: BASE_DIR)
â””â”€â”€ README.md                  # Project documentation (French version)
```

ğŸ” Key configuration files
| File                | Role                                                              |
| ------------------- | ----------------------------------------------------------------- |
| `docker-compose.yml`| Launches all required services (Airflow, API, MLflow, etc.)       |
| `params.yaml`       | Centralizes hyperparameters, paths, splits, etc.                  |
| `processed.dvc`     | Tracks data transformations via DVC                               |
| `.dvcignore`        | Excludes files from DVC tracking                                  |
| `.env`              | Defines Docker environment variables (base_dir, etc.)             |

<br>To generate the `docker-compose.yml` file from the `docker-compose-template.yml`, run:
```
./generate_compose.sh
```

---
### ğŸ§° Services
| Service     | Port  | Description                           |
| ----------- | ----- | ------------------------------------- |
| Airflow UI  | 8080  | Pipeline orchestration                |
| MLflow      | 5000  | Experiment tracking                   |
| API FastAPI | 8000  | Prediction endpoint                   |
| Evidently   | 9000  | Data drift monitoring                 |
| Prometheus  | 90900 | API metrics monitoring                |
| Grafana     | 30300 | Prometheus metrics visualization      |
| Streamlit   | 8501  | Streamlit project app                 |

---
### â–¶ï¸ Start the environment
1. Prerequisites

   - Docker installed  
   - Create a `.env` file based on `.env_template`  

2. Launch the services
   ```
   ./scripts/deploy_k3s.sh        # Deploy API on K3S pod
   ./scripts/deploy_monitoring.sh # Deploy API monitoring with Prometheus/Grafana
   docker compose up --build      # Launch other services (Airflow, MLflow, Evidently, Streamlit)
   ```

   - Airflow â†’ [http://localhost:8080](http://localhost:8080)  
   - MLflow â†’ [http://localhost:5000](http://localhost:5000)  
   - Prometheus â†’ [http://localhost:90900](http://localhost:90900) (metrics from API & Airflow via `/metrics`)  
   - Grafana â†’ [http://localhost:30300](http://localhost:30300) (default login: `admin / admin`)  

---
### âš™ï¸ Airflow Pipelines

The main DAG (`rakuten_dags.py`) orchestrates the following steps:
- data_loading_task  
- preprocessing_task  
- training_task  
- evaluation_task  
- mlflow_dag_task  

---
### ğŸ§ª Experiment Tracking

Training logs and artifacts are automatically tracked in MLflow:

```http://localhost:5000 ```

Tracking includes:
- evaluation metrics  
- model parameters  
- visualizations  

---
### ğŸŒ Prediction API

A FastAPI service exposes a prediction endpoint on port **8000**.

---
### ğŸ§ª Testing & Reproducibility

Each component is encapsulated in a dedicated Docker image (dataloading, preprocessing, etc.), ensuring isolation and reproducibility.

---
### ğŸ—‚ï¸ Data management with DVC

Data transformations are tracked with **DVC**, enabling dataset versioning:
```
dvc repro
dvc push
``` 

---
### ğŸ“Š Monitoring with Prometheus & Grafana

The FastAPI app is instrumented with `prometheus_fastapi_instrumentator` to expose metrics:

```
http://localhost:8000/metrics
```

ğŸ” Prometheus

Prometheus scrapes API metrics every 15 seconds.  
UI accessible at:

```
http://localhost:90900
```

Example PromQL query:

```
sum by (handler) (http_requests_total)
```

This shows the total number of requests per endpoint.

ğŸ“ˆ Grafana

Grafana allows dashboards creation from Prometheus data.  
Access:

```
http://localhost:30300
```

Default credentials:  
- Login: `admin`  
- Password: `admin`  

To configure:  
1. Go to â€œConnections > Data sourcesâ€  
2. Click â€œAdd data sourceâ€  
3. Select **Prometheus**  
4. Set URL: `http://prometheus:90900`  
5. Create panels with PromQL queries (e.g., `http_requests_total`)  

---
### âœ… Unit tests

The `tests/` directory contains unit tests to validate pipeline steps: data loading, preprocessing, training, prediction, etc.  

ğŸ“¦ Structure
```
tests/
â”œâ”€â”€ test_dataloading.py
â”œâ”€â”€ test_preprocessing.py
â”œâ”€â”€ test_training.py
â”œâ”€â”€ test_evaluation.py
â””â”€â”€ conftest.py  # Shared fixtures
```

â–¶ï¸ Run tests

Make sure your EC2 instance with GPU is running, then execute:
```
./scripts/run_ci.sh
```

---
### K3s installation
Run:
```
./scripts/install_k3s.sh
```

### Deploy API & monitoring on K3s
Run:
```
./scripts/deploy_k3s.sh
./scripts/deploy_monitoring.sh
```

### Clean up K3s resources (namespace, pv/pvc, pods)
Run:
```
./scripts/cleanup_monitoring.sh
./scripts/cleanup_k3s.sh
```

---
### ğŸ“ Authors

- Olivier ISNARD  
- Christian SEGNOU  

Supervised as part of the MLOps program by Maria (DataScientest).
