services:
  init-dataset:
    build:
      context: .
      dockerfile: docker/Dockerfile.dataloading
    container_name: init-dataset
    working_dir: /app/src/data
    volumes:
      - .:/app
      - ./data:/opt/airflow/data
    command: python make_dataset.py
    depends_on:
      - postgres
    restart: "no"
    environment:
      - PYTHONPATH=/app

  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    restart: always

  redis:
    image: redis:latest
    restart: always

  airflow-webserver:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
      args:
        DOCKER_GID: ${DOCKER_GID}
    image: custom_airflow:2.9.1
    env_file: .env
    restart: always
    depends_on:
      - postgres
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: aed2e9108e5e161ca37c111e14b1c3f9f4d307f79f2c58ec4993fd37e64c2815
      AIRFLOW__WEBSERVER__BASE_URL: http://localhost:8080
      BASE_DIR: ${BASE_DIR}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8080:8080"
    command: >
      bash -c "
        airflow db upgrade &&
        airflow users create --username admin --password admin --firstname Air --lastname Flow --role Admin --email admin@example.com &&
        airflow webserver
      "

  airflow-scheduler:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
      args:
        DOCKER_GID: ${DOCKER_GID}
    image: custom_airflow:2.9.1
    env_file: .env
    restart: always
    depends_on:
      - airflow-webserver
      - init-dataset
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: aed2e9108e5e161ca37c111e14b1c3f9f4d307f79f2c58ec4993fd37e64c2815
      AIRFLOW__WEBSERVER__BASE_URL: http://localhost:8080
      BASE_DIR: ${BASE_DIR}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
    command: scheduler

  airflow-worker:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
      args:
        DOCKER_GID: ${DOCKER_GID}
    image: custom_airflow:2.9.1
    env_file: .env
    restart: always
    depends_on:
      - airflow-webserver
      - init-dataset
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: aed2e9108e5e161ca37c111e14b1c3f9f4d307f79f2c58ec4993fd37e64c2815
      AIRFLOW__WEBSERVER__BASE_URL: http://localhost:8080
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
    command: celery worker

  airflow-init:
    image: apache/airflow:2.9.1
    container_name: airflow-init
    env_file: .env
    entrypoint: >
      bash -c "
        airflow db init &&
        airflow users create --username admin --password admin --firstname Air --lastname Flow --role Admin --email admin@example.com
      "
    depends_on:
      - postgres
      - redis
      - init-dataset
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: aed2e9108e5e161ca37c111e14b1c3f9f4d307f79f2c58ec4993fd37e64c2815
      AIRFLOW__WEBSERVER__BASE_URL: http://localhost:8080
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
      - ./src:/opt/airflow/src
      - airflow_data:/opt/airflow
    restart: on-failure

  dataloading:
    build:
      context: .
      dockerfile: docker/Dockerfile.dataloading
    image: mai25_cmlops_rakuten_dataloading
    volumes:
      - ./data:/app/data
      - ./src:/app/src

  preprocessing:
    build:
      context: .
      dockerfile: docker/Dockerfile.preprocessing
    image: mai25_cmlops_rakuten_preprocessing
    volumes:
      - ./data:/app/data
      - ./src:/app/src

  make_datastreams:
    build:
      context: .
      dockerfile: docker/Dockerfile.make_datastreams
    image: mai25_cmlops_rakuten_make_datastreams
    #mai25_cmlops_rakuten_make_datastreams
    volumes:
      - ./data:/app/data
      - ./src:/app/src


  preprocessing_datastreams:
    build:
      context: .
      dockerfile: docker/Dockerfile.preprocessing_datastreams
    image: mai25_cmlops_rakuten_preprocessing_datastreams
    volumes:
      - ./data:/app/data
      - ./src:/app/src

  training:
    build:
      context: .
      dockerfile: ${DOCKERFILE_TRAIN}
    image: mai25_cmlops_rakuten_train
    volumes:
      - ./data:/app/data
      - ./src:/app/src
      - ./models:/app/models
      - ./mlruns:/app/mlruns

  evaluation:
    build:
      context: .
      dockerfile: ${DOCKERFILE_EVALUATE}
      # docker/Dockerfile.evaluate
    image: mai25_cmlops_rakuten_evaluate
    volumes:
      - ./data:/app/data
      - ./src:/app/src
      - ./models:/app/models
      - ./metrics:/app/metrics

  mlflow:
    build:
      context: .
      dockerfile: docker/Dockerfile.mlflow
    image: mai25_cmlops_rakuten_mlflow
    ports:
      - 5000:5000
    volumes:
      - ./mlruns:/app/mlruns
    environment:
      MLFLOW_TRACKING_URI: http://0.0.0.0:5000
    depends_on:
      - init-dataset

# api moved to k8s for better scalability
#  api:
#    build:
#      context: .
#      dockerfile: ${DOCKERFILE_API}
#    image: mai25_cmlops_rakuten_api
#    ports:
#      - "8000:8000"
#    volumes:
#      - ./data:/app/data/
#      - ./models:/app/models
#      - ./src:/app/src
#      - ./tests:/app/tests
#    networks:
#      - mai25
#    depends_on:
#      - init-dataset

  build_features:
    build:
      context: .
      dockerfile: ${DOCKERFILE_FEATURES}
    image: mai25_cmlops_rakuten_features
    volumes:
      - ./data:/app/data
      - ./src:/app/src
      - ./models:/app/models
      - ./mlruns:/app/mlruns

  evidently_reports:
    build:
      context: .
      dockerfile: docker/Dockerfile.evidently
    image: mai25_cmlops_rakuten_evidently
    volumes:
      - ./data:/app/data
      - ./src:/app/src
      - ./monitoring:/app/monitoring
      - ./evidently_workspace:/app/evidently_workspace

  evidenty_server:
    build:
      context: .
      dockerfile: docker/Dockerfile.evidently
    image: mai25_cmlops_rakuten_evidently_server
    ports:
      - "9000:9000"
    volumes:
      - ./evidently_workspace:/app/evidently_workspace
    command: >
      bash -c "
        evidently ui --workspace evidently_workspace --port 9000 --host 0.0.0.0
      "
    depends_on:
      - evidently_reports
      - init-dataset

# Prometheus & Grafana for monitoring (optional) --> moved to k8s for better scalability
#  prometheus:
#    image: prom/prometheus:latest
#    container_name: prometheus
#    volumes:
#      - ./docker/prometheus.yml:/etc/prometheus/prometheus.yml
#    ports:
#      - "9090:9090"
#    networks:
#      - mai25
#    depends_on:
#      - init-dataset

#  grafana:
#    image: grafana/grafana:latest
#    container_name: grafana
#    ports:
#      - "3000:3000"
#    volumes:
#      - grafana-storage:/var/lib/grafana
#      - ./docker/grafana/provisionning/datasources:/etc/grafana/provisioning/datasources
#      - ./docker/grafana/provisionning/dashboard:/etc/grafana/provisioning/dashboards
#      - ./docker/grafana/provisionning/dashboard:/var/lib/grafana/dashboards
#    networks:
#      - mai25
#    depends_on:
#      - prometheus

  traffic-generator:
    build:
      context: .
      dockerfile: docker/Dockerfile.traffic_generator  # Nom du Dockerfile si ce n'est pas le nom par défaut
    image: rakuten-traffic-gen
    container_name: traffic-generator
    volumes:
#      - /etc/rancher/k3s/k3s.yaml:/root/.kube/config:ro  # Pour accéder à K3s depuis le container
      - ./data:/app/data
      - ./src:/app/src
      - ./.env:/app/.env:ro
    environment:
      - INGRESS_IP=http://192.168.1.35
#      - KUBECONFIG=/root/.kube/config
    restart: "no"

volumes:
  postgres-db-volume:
  airflow_data:
  airflow_logs:
  airflow_plugins:
  data:
  models:
  mlruns:
#  grafana-storage:

networks:
  mai25:
    driver: bridge
